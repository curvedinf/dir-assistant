============================= test session starts ==============================
platform linux -- Python 3.12.5, pytest-8.4.0, pluggy-1.6.0
rootdir: /home/chase/Projects/dir-assistant
plugins: cov-6.2.1, anyio-4.9.0, asyncio-1.1.0, django-4.11.1
asyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 6 items

test/test_artifact_cutoff.py .F
test/test_git_assistant.py Apply these changes? (Y/N): 
.Apply these changes? (Y/N): 
.
test/test_smoketest_interactive.py dir-assistant 1.7.0
Released under MIT License
https://github.com/curvedinf/dir-assistant

[90mLoading embedding model...
[0m[90mCreating file embeddings and index...
[0mUsing cached embeddings for /home/chase/Projects/dir-assistant/smoketest.sh
Using cached embeddings for /home/chase/Projects/dir-assistant/CONTRIBUTORS.md
Using cached embeddings for /home/chase/Projects/dir-assistant/setup.py
Using cached embeddings for /home/chase/Projects/dir-assistant/format-code.sh
Using cached embeddings for /home/chase/Projects/dir-assistant/publish-test.sh
Using cached embeddings for /home/chase/Projects/dir-assistant/install-local.sh
Using cached embeddings for /home/chase/Projects/dir-assistant/LICENSE
Creating embeddings for /home/chase/Projects/dir-assistant/test_output.txt
Using cached embeddings for /home/chase/Projects/dir-assistant/README.md
Using cached embeddings for /home/chase/Projects/dir-assistant/publish.sh
Using cached embeddings for /home/chase/Projects/dir-assistant/docs/configuration.md
Using cached embeddings for /home/chase/Projects/dir-assistant/docs/project_info.md
Using cached embeddings for /home/chase/Projects/dir-assistant/docs/usage.md
Using cached embeddings for /home/chase/Projects/dir-assistant/docs/install.md
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/__main__.py
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/__init__.py
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/main.py
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/cli/models.py
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/cli/start.py
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/cli/__init__.py
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/cli/platform_setup.py
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/cli/setkey.py
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/cli/config.py
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/assistant/base_assistant.py
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/assistant/cgrag_assistant.py
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/assistant/lite_llm_assistant.py
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/assistant/file_watcher.py
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/assistant/git_assistant.py
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/assistant/rag_optimizer.py
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/assistant/llama_cpp_embed.py
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/assistant/index.py
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/assistant/__init__.py
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/assistant/base_embed.py
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/assistant/lite_llm_embed.py
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/assistant/cache_manager.py
Using cached embeddings for /home/chase/Projects/dir-assistant/dir_assistant/assistant/llama_cpp_assistant.py
Using cached embeddings for /home/chase/Projects/dir-assistant/test/test_git_assistant.py
Using cached embeddings for /home/chase/Projects/dir-assistant/test/test_artifact_cutoff.py
Using cached embeddings for /home/chase/Projects/dir-assistant/test/test_smoketest_noninteractive.py
Using cached embeddings for /home/chase/Projects/dir-assistant/test/utils.py
Using cached embeddings for /home/chase/Projects/dir-assistant/test/__init__.py
Using cached embeddings for /home/chase/Projects/dir-assistant/test/test_smoketest_interactive.py
Using cached embeddings for /home/chase/Projects/dir-assistant/examples/reddit-stock-sentiment.sh
Creating index from embeddings...
[90mLoading remote LLM model...
[0m[90mLiteLLM completion options: {'model': 'openrouter/openrouter/sonoma-sky-alpha', 'timeout': 600}[0m
[90mLiteLLM context size: 10000[0m
[90mLiteLLM CGRAG completion options: {'model': 'openrouter/openrouter/sonoma-dusk-alpha', 'timeout': 600}[0m
[90mLiteLLM CGRAG context size: 70000[0m
[1m[32m
  _____ _____ _____                                              
 |  __ \_   _|  __ \
 | |  | || | | |__) |
 | |  | || | |  _  /
 | |__| || |_| | \ \
 |_____/_____|_|_ \_\__ _____  _____ _______       _   _ _______ 
     /\    / ____/ ____|_   _|/ ____|__   __|/\   | \ | |__   __|
    /  \  | (___| (___   | | | (___    | |  /  \  |  \| |  | |   
   / /\ \  \___ \\___ \  | |  \___ \   | | / /\ \ | . ` |  | |   
  / ____ \ ____) |___) |_| |_ ____) |  | |/ ____ \| |\  |  | |   
 /_/    \_\_____/_____/|_____|_____/   |_/_/    \_\_| \_|  |_|   
[0m
[1m[34mType 'exit' to quit the conversation.
[1m[34mType 'undo' to roll back the last commit.

[1m[31mYou (Press ALT-Enter, OPT-Enter, or CTRL-O to submit): 

[0m[?12l[?25h[6n[?2004h[?1l[?25l[0m[?7l[0m[J[0m [?7h[0m[?12l[?25h[?25l[?7l[0mDescribe the purpose of this codebase.[0m
[0m [0m
[0mThen say roughly how many lines of code it has.[47D[47C[?7h[0m[?12l[?25h[?25l[?7l[2A[47D[0m[J[0mDescribe the purpose of this codebase.[0m
[0m [0m
[0mThen say roughly how many lines of code it has.[47D[0m
[J[?7h[0m[?12l[?25h[?2004l
[1m[33mFile changes detected. Index has been updated.[0m
[1m[31mYou (Press ALT-Enter, OPT-Enter, or CTRL-O to submit): 

[0m.
test/test_smoketest_noninteractive.py ### What Does This Codebase Do?

The `dir-assistant` codebase is a Python-based command-line interface (CLI) tool designed to enable users to "chat" with the contents of their local directory (e.g., a code project or file system) using large language models (LLMs). It acts as an AI-powered assistant for tasks like code analysis, debugging, automation, and querying files without manually copying and pasting content into an LLM prompt. Here's a breakdown of its core purpose and functionality, based on the provided files:

#### Core Purpose
- **File Indexing and Retrieval-Augmented Generation (RAG)**: It recursively scans and indexes text-based files in your current working directory (or specified directories). When you ask a question (e.g., "Explain this codebase" or "Fix this bug"), it uses embeddings (vector representations of file chunks) to retrieve the most relevant file snippets and injects them into the LLM prompt. This ensures the AI responds with context from your actual files, making it ideal for developers working on large codebases.
- **AI Interaction Modes**:
  - **Interactive Chat Mode**: Run `dir-assistant` to enter a conversational loop where you can ask questions, and the tool streams responses from the LLM while dynamically including file context.
  - **Single-Prompt Mode**: Use `-s "your query"` for non-interactive, one-off queries (e.g., in scripts or automation), outputting just the final answer.
- **Target Use Cases**: Primarily for coding assistance (e.g., summarizing code, generating tests, refactoring), but extensible to any text-heavy directory. It supports automation, like auto-committing file changes to Git when enabled.

#### Key Components and How It Works
1. **Indexing and Search (from `assistant/index.py` and `base_assistant.py`)**:
   - Builds a searchable index of file chunks using embedding models (e.g., nomic-embed-text-v1.5).
   - Supports parallelized indexing for speed.
   - Uses FAISS (via `llama-cpp-python`) for efficient similarity search.
   - Indexes are now separated by embedding model, allowing switches without re-indexing.

2. **Context Optimization with CGRAG (from `base_assistant.py` and `rag_optimizer.py`)**:
   - Employs a unique "CGRAG" (Contextually Guided Retrieval-Augmented Generation) method to select and prioritize file chunks. This goes beyond basic RAG by:
     - Pre-culling candidates based on token limits.
     - Optimizing for caching (e.g., reusing frequent or recently modified artifacts).
     - Balancing relevance, history, and recency to fit within LLM context windows (e.g., 8192 tokens by default, adjustable up to 200k+ for APIs).
   - Includes features like artifact metadata tracking, prefix caching for repeated queries, and relevancy cutoffs to avoid irrelevant noise.

3. **LLM Integration (via `base_assistant.py` and CLI modules)**:
   - **Local LLMs**: Supports running models like Phi-3.1-mini on hardware like CPU (OpenBLAS), NVIDIA CUDA, AMD ROCm, Apple Metal, Intel SYCL, or Vulkan. Downloads models via `dir-assistant models download-llm`.
   - **API LLMs**: Integrates with providers like Google Gemini, Anthropic Claude, OpenAI GPT-4o, and more via LiteLLM. Set keys with `dir-assistant setkey` (e.g., `GEMINI_API_KEY`).
   - Handles streaming responses, thinking indicators (e.g., "(thinking...)"), and output filtering (e.g., hiding internal reasoning).
   - Configurable for separate "guidance" LLMs to optimize costs/latency.

4. **CLI Entry Point (from `main.py`)**:
   - Uses `argparse` for commands like:
     - `start`: Launch chat or single-prompt mode (with options for ignore paths, directories, verbosity, no-color).
     - `platform [cpu|cuda|rocm|...]`: Set up hardware acceleration.
     - `config [print|open]`: Manage settings (e.g., context size, model options in TOML format).
     - `models [download-embed|download-llm|open|print]`: Handle model downloads and directories.
     - `clear`: Reset index cache (useful for upgrades).
     - `setkey`: Store API keys securely.
   - Loads config from a TOML file and supports verbose logging.

5. **Additional Features**:
   - **File Watching and Updates**: Detects changes (e.g., via `update_index_and_chunks`) and rebuilds the index dynamically.
   - **Caching and History**: Manages prompt history, prefix hits, and API context TTL to reduce redundancy and costs.
   - **Git Integration**: Optionally auto-commits changes during interactions.
   - **Error Handling and UX**: Colored output (via Colorama), debug messages, and retries for rejected outputs.
   - **Limitations**: Only handles text files (no binaries/images); Windows local LLM support is indirect (recommend LM Studio as a server).

#### Architecture Overview
- **Modular Structure**:
  - `cli/`: Handles commands, config, models, and setup.
  - `assistant/`: Core logic (e.g., `BaseAssistant` for chat flow, `CacheManager` for persistence, `RagOptimizer` for context selection).
  - Dependencies: `llama-cpp-python` (local LLMs/embeddings), `LiteLLM` (APIs), `numpy`, `faiss`, `colorama`, `toml`.
- **Data Flow**: User input → Embed query → Retrieve/optimize chunks → Build prompt with system instructions + context + query → Call LLM → Stream/filter response → Update cache/index.
- **Version and Licensing**: Released under MIT; current version info in `cli/config.py`. Tracks contributors (e.g., curvedinf as lead).

#### Getting Started (from README)
- Install: `pip install dir-assistant[recommended]` (or `pipx` on Ubuntu 24.04).
- Local: Download models, run `dir-assistant` in your dir.
- API: Set key (e.g., for free Gemini), then chat.
- Docs: See `docs/` for install, usage, config, and project info.

In summary, this codebase turns your directory into an AI-queryable knowledge base, leveraging advanced RAG techniques to make LLMs context-aware of your files. It's optimized for developer workflows, with flexibility for local/offline use or cloud APIs. If you have a specific file or feature in mind, I can dive deeper!

.

=================================== FAILURES ===================================
______ TestArtifactRelevancyCutoff.test_cgrag_assistant_relevancy_cutoffs ______

self = <test.test_artifact_cutoff.TestArtifactRelevancyCutoff testMethod=test_cgrag_assistant_relevancy_cutoffs>
mock_search_index = <MagicMock name='search_index' id='133217385502416'>

    @patch('dir_assistant.assistant.base_assistant.search_index')
    def test_cgrag_assistant_relevancy_cutoffs(self, mock_search_index):
        # Configure mock
        mock_search_index.return_value = self.mock_search_results
        # Instantiate assistant with different cutoffs for regular and CGRAG
        assistant = CGRAGAssistant(
            system_instructions="test",
            embed=self.mock_embed,
            index=self.mock_index,
            chunks=self.all_chunks,
            context_file_ratio=0.8,
            artifact_excludable_factor=0.1,
            artifact_relevancy_cutoff=1.5,
            artifact_relevancy_cgrag_cutoff=1.2, # Stricter cutoff for CGRAG
            api_context_cache_ttl=3600,
            rag_optimizer_weights={},
            output_acceptance_retries=1,
            use_cgrag=True,
            print_cgrag=False,
            verbose=False,
            no_color=True,
            chat_mode=False,
            hide_thinking=True,
            thinking_start_pattern="",
            thinking_end_pattern="",
        )
        assistant.context_size = 1000
        assistant.count_tokens = MagicMock(return_value=10)
        # Test CGRAG cutoff (1.2)
        cgrag_text = assistant.build_relevant_full_text("test query for cgrag", cutoff=assistant.artifact_relevancy_cgrag_cutoff)
        self.assertIn("chunk1_text", cgrag_text)
>       self.assertNotIn("chunk2_text", cgrag_text)
E       AssertionError: 'chunk2_text' unexpectedly found in 'chunk1_text\n\n\n\nchunk2_text\n\n\n\n'

test/test_artifact_cutoff.py:91: AssertionError
=============================== warnings summary ===============================
../../.pyenv/versions/deus-stulti/lib/python3.12/site-packages/faiss/loader.py:49
  /home/chase/.pyenv/versions/deus-stulti/lib/python3.12/site-packages/faiss/loader.py:49: DeprecationWarning: numpy.core._multiarray_umath is deprecated and has been renamed to numpy._core._multiarray_umath. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core._multiarray_umath.__cpu_features__.
    from numpy.core._multiarray_umath import __cpu_features__

<frozen importlib._bootstrap>:488
  <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute

<frozen importlib._bootstrap>:488
  <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute

<frozen importlib._bootstrap>:488
  <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type swigvarlink has no __module__ attribute

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED test/test_artifact_cutoff.py::TestArtifactRelevancyCutoff::test_cgrag_assistant_relevancy_cutoffs
=================== 1 failed, 5 passed, 4 warnings in 17.52s ===================
