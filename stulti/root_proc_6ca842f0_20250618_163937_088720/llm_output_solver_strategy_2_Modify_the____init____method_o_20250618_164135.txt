--- DEUS STULTI ARTIFACT ---
Timestamp: 2025-06-18T16:41:35.759368
Git Branch: feature_cgrag-llm-config
----------------------------

--- Request Description ---
Modify the `__init__` method of `dir_assistant/assistant/lite_llm_assistant.py` to accept new parameters for `cgrag_lite_llm_completion_options`, `cgrag_lite_llm_context_size`, and `cgrag_lite_llm_pass_through_context_size`. Store these new parameters as instance variables for later use.
--- LLM Response ---
{'strategy': 'TOOL', 'payload': {'tool_name': 'execute_shell_script', 'arguments': {'script_content': 'cat << \'EOF\' > /home/chase/Projects/dir-assistant/dir_assistant/assistant/lite_llm_assistant.py\nfrom copy import deepcopy\nfrom time import sleep\nfrom colorama import Fore, Style\nfrom litellm import completion\nfrom litellm import exceptions as litellm_exceptions\nfrom litellm import token_counter\nfrom dir_assistant.assistant.git_assistant import GitAssistant\nclass LiteLLMAssistant(GitAssistant):\n    def __init__(\n        self,\n        lite_llm_completion_options,\n        lite_llm_context_size,\n        lite_llm_model_uses_system_message,\n        lite_llm_pass_through_context_size,\n        cgrag_lite_llm_completion_options,\n        cgrag_lite_llm_context_size,\n        cgrag_lite_llm_pass_through_context_size,\n        system_instructions,\n        embed,\n        index,\n        chunks,\n        context_file_ratio,\n        output_acceptance_retries,\n        use_cgrag,\n        print_cgrag,\n        commit_to_git,\n        verbose,\n        no_color,\n        chat_mode,\n        hide_thinking,\n        thinking_start_pattern,\n        thinking_end_pattern,\n    ):\n        super().__init__(\n            system_instructions,\n            embed,\n            index,\n            chunks,\n            context_file_ratio,\n            output_acceptance_retries,\n            use_cgrag,\n            print_cgrag,\n            commit_to_git,\n            verbose,\n            no_color,\n            chat_mode,\n            hide_thinking,\n            thinking_start_pattern,\n            thinking_end_pattern,\n        )\n        self.completion_options = lite_llm_completion_options\n        self.context_size = lite_llm_context_size\n        self.pass_through_context_size = lite_llm_pass_through_context_size\n        self.lite_llm_model_uses_system_message = lite_llm_model_uses_system_message\n        self.cgrag_completion_options = cgrag_lite_llm_completion_options\n        self.cgrag_context_size = cgrag_lite_llm_context_size\n        self.cgrag_pass_through_context_size = cgrag_lite_llm_pass_through_context_size\n        self.no_color = no_color\n        if self.chat_mode and self.verbose:\n            if self.no_color:\n                print(f"LiteLLM context size: {self.context_size}")\n            else:\n                print(\n                    f"{Fore.LIGHTBLACK_EX}LiteLLM context size: {self.context_size}{Style.RESET_ALL}"\n                )\n    def initialize_history(self):\n        super().initialize_history()\n        if not self.lite_llm_model_uses_system_message:\n            if self.chat_history and self.chat_history[0]["role"] == "system":\n                self.chat_history[0]["role"] = "user"\n    def call_completion(self, chat_history):\n        # Clean "tokens" from chat history. It causes an error for mistral.\n        chat_history_cleaned = deepcopy(chat_history)\n        for message in chat_history_cleaned:\n            if "tokens" in message:\n                del message["tokens"]\n        # Hardcoded retry settings\n        max_retries = 3\n        retry_delay_seconds = 1\n        current_retry = 0\n        while current_retry <= max_retries:\n            try:\n                if self.pass_through_context_size:\n                    return completion(\n                        **self.completion_options,\n                        messages=chat_history_cleaned,\n                        stream=True,\n                        num_ctx=self.context_size,\n                    )\n                else:\n                    return completion(\n                        **self.completion_options,\n                        messages=chat_history_cleaned,\n                        stream=True,\n                    )\n            except litellm_exceptions.APIConnectionError as e:\n                current_retry += 1\n                if current_retry > max_retries:\n                    self.write_error_message(\n                        f"API Connection Error after {max_retries} retries: {e}"\n                    )\n                    raise\n                self.write_debug_message(\n                    f"API Connection Error (attempt {current_retry}/{max_retries}): {e}. Retrying in {retry_delay_seconds}s..."\n                )\n                sleep(retry_delay_seconds)\n            except Exception as e:  # Catch other potential litellm exceptions\n                self.write_error_message(f"LiteLLM completion error: {e}")\n                raise\n        # This line should ideally not be reached if the loop logic is correct (always returns or raises).\n        # Added for robustness in case of unforeseen loop exit.\n        raise Exception(\n            f"[dir-assistant] LiteLLMAssistant Error: Completion failed "\n            "after exhausting retries or due to an unhandled state."\n        )\n    def count_tokens(self, text, role="user"):\n        valid_roles = ["system", "user", "assistant"]\n        role_to_pass = role\n        if role not in valid_roles:\n            self.write_debug_message(\n                f"count_tokens received invalid role \'{role}\', defaulting to \'user\'."\n            )\n            role_to_pass = "user"\n        return token_counter(\n            model=self.completion_options["model"],\n            messages=[{"role": role_to_pass, "content": text}],\n        )\n\nEOF'}}}


--- Result ---
Status: success
Output: {'status': 'success', 'stdout': '', 'stderr': ''}