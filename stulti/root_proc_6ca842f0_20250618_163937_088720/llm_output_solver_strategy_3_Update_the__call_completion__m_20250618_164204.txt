--- DEUS STULTI ARTIFACT ---
Timestamp: 2025-06-18T16:42:04.056267
Git Branch: feature_cgrag-llm-config
----------------------------

--- Request Description ---
Update the `call_completion` method signature in `dir_assistant/assistant/base_assistant.py` to include a new boolean parameter `is_cgrag_call` with a default value of `False`. This allows subclasses to indicate if a call is specifically for CGRAG processing.
--- LLM Response ---
{'strategy': 'TOOL', 'payload': {'tool_name': 'execute_shell_script', 'arguments': {'script_content': 'cat << \'EOF\' > /home/chase/Projects/dir-assistant/dir_assistant/assistant/base_assistant.py\nimport sys\nimport numpy as np\nfrom colorama import Fore, Style\nfrom dir_assistant.assistant.index import search_index\nclass BaseAssistant:\n    """\n    A base class for LLM assistants that enables inclusion of local files in the LLM context. Files\n    are collected recursively from the current directory.\n    """\n    def __init__(\n        self,\n        system_instructions,\n        embed,\n        index,\n        chunks,\n        context_file_ratio,\n        output_acceptance_retries,\n        verbose,\n        no_color,\n        chat_mode,\n        hide_thinking,\n        thinking_start_pattern,\n        thinking_end_pattern,\n    ):\n        self.system_instructions = system_instructions\n        self.embed = embed\n        self.index = index\n        self.chunks = chunks\n        self.context_file_ratio = context_file_ratio\n        self.context_size = 8192\n        self.output_acceptance_retries = output_acceptance_retries\n        self.no_color = no_color\n        self.verbose = verbose\n        self.chat_mode = chat_mode\n        self.hide_thinking = hide_thinking\n        self.thinking_start_pattern = thinking_start_pattern\n        self.thinking_end_pattern = thinking_end_pattern\n    def initialize_history(self):\n        # This inititialization occurs separately from the constructor because child classes need to initialize\n        # before count_tokens can be called.\n        system_instructions_tokens = self.count_tokens(\n            self.system_instructions, role="system"\n        )\n        self.chat_history = [\n            {\n                "role": "system",\n                "content": self.system_instructions,\n                "tokens": system_instructions_tokens,\n            }\n        ]\n    def call_completion(self, chat_history, is_cgrag_call=False):\n        # unimplemented on base class\n        raise NotImplementedError\n    def run_completion_generator(\n        self, completion_output, output_message, write_to_stdout\n    ):\n        # unimplemented on base class\n        raise NotImplementedError\n    def count_tokens(self, text, role="user"):\n        # unimplemented on base class\n        raise NotImplementedError\n    def build_relevant_full_text(self, user_input):\n        relevant_chunks = search_index(self.embed, self.index, user_input, self.chunks)\n        relevant_full_text = ""\n        chunk_total_tokens = 0\n        for i, relevant_chunk in enumerate(relevant_chunks, start=1):\n            # Note: relevant_chunk["tokens"] is created with the embedding model, not the LLM, so it will\n            # not be accurate for the purposes of maximizing the context of the LLM.\n            chunk_total_tokens += self.count_tokens(\n                relevant_chunk["text"] + "\\n\\n", role="user"\n            )  # Assuming RAG text is like user content for token counting\n            if chunk_total_tokens >= self.context_size * self.context_file_ratio:\n                break\n            relevant_full_text += relevant_chunk["text"] + "\\n\\n"\n        return relevant_full_text\n    def get_color_prefix(self, style=None, fore=None):\n        if self.no_color:\n            return ""\n        result = ""\n        if style:\n            result += style\n        if fore:\n            result += fore\n        return result\n    def get_color_suffix(self):\n        return "" if self.no_color else Style.RESET_ALL\n    def write_assistant_thinking_message(self):\n        if self.chat_mode:\n            color_prefix = self.get_color_prefix(Style.BRIGHT, Fore.GREEN)\n            color_suffix = self.get_color_suffix()\n            sys.stdout.write(f"{color_prefix}\\nAssistant: \\n\\n{color_suffix}")\n            sys.stdout.write(\n                f"{self.get_color_prefix(Style.BRIGHT, Fore.WHITE)}\\r(thinking...){color_suffix}"\n            )\n            sys.stdout.flush()\n    def write_error_message(self, message):\n        if self.chat_mode:\n            color_prefix = self.get_color_prefix(Style.BRIGHT, Fore.RED)\n            color_suffix = self.get_color_suffix()\n            sys.stdout.write(f"{color_prefix}{message}{color_suffix}\\n")\n            sys.stdout.flush()\n    def write_debug_message(self, message):\n        if self.verbose and self.chat_mode:\n            color_prefix = self.get_color_prefix(Style.BRIGHT, Fore.YELLOW)\n            color_suffix = self.get_color_suffix()\n            sys.stdout.write(f"{color_prefix}Debug: {message}{color_suffix}\\n")\n            sys.stdout.flush()\n    def create_user_history(self, temp_content, final_content):\n        return {\n            "role": "user",\n            "content": temp_content,\n            "tokens": self.embed.count_tokens(\n                final_content\n            ),  # This uses embed\'s token counter\n        }\n    def add_user_history(self, temp_content, final_content):\n        self.chat_history.append(self.create_user_history(temp_content, final_content))\n    def cull_history(self):\n        self.cull_history_list(self.chat_history)\n    def find_truncation_point(self, text, target_tokens, role="user"):\n        # Helper to find how much of \'text\' fits into \'target_tokens\'\n        low = 0\n        high = len(text)\n        best_fit_text = ""\n        if self.count_tokens("", role=role) > target_tokens:\n            return ""\n        while low <= high:\n            mid = (low + high) // 2\n            substring = text[:mid]\n            tokens = self.count_tokens(substring, role=role)\n            if tokens <= target_tokens:\n                best_fit_text = substring\n                low = mid + 1\n            else:\n                high = mid - 1\n        return best_fit_text\n    def cull_history_list(self, history_list):\n        current_total_tokens = sum(\n            self.count_tokens(message["content"], role=message["role"])\n            for message in history_list\n        )\n        # Phase 1: Pop older messages (all but the last one ideally) if oversized\n        while current_total_tokens > self.context_size and len(history_list) > 1:\n            popped_message = history_list.pop(0)\n            current_total_tokens -= self.count_tokens(\n                popped_message["content"], role=popped_message["role"]\n            )\n            self.write_debug_message(\n                f"Context too large. Dropped older message (role: {popped_message[\'role\']}) to free up space."\n            )\n        # Phase 2: If still oversized and only one message remains (the latest prompt), truncate it.\n        if current_total_tokens > self.context_size and len(history_list) == 1:\n            last_message = history_list[0]\n            original_content = last_message["content"]\n            original_tokens = self.count_tokens(\n                original_content, role=last_message["role"]\n            )\n            # The available tokens for this last message is based on the file ratio times the context size\n            available_tokens_for_last_message = self.context_size - (\n                self.context_size * self.context_file_ratio\n            )\n            if original_tokens > available_tokens_for_last_message:\n                self.write_error_message(\n                    f"\\nLatest prompt is too large ({original_tokens} tokens) for history context ({available_tokens_for_last_message} tokens). Truncating it."\n                )\n                truncated_content = self.find_truncation_point(\n                    original_content,\n                    available_tokens_for_last_message,\n                    role=last_message["role"],\n                )\n                last_message["content"] = truncated_content\n                new_tokens_for_last_message = self.count_tokens(\n                    truncated_content, role=last_message["role"]\n                )\n                last_message["tokens"] = (\n                    new_tokens_for_last_message  # Update field if used, though culling relies on self.count_tokens\n                )\n                current_total_tokens = (\n                    new_tokens_for_last_message  # As it\'s the only message\n                )\n                if len(truncated_content) < len(original_content):\n                    self.write_debug_message(\n                        f"Latest message truncated to fit. Original length: {len(original_content)} chars, new length: {len(truncated_content)} chars. New tokens: {new_tokens_for_last_message}"\n                    )\n                elif not truncated_content and original_content:  # Content became empty\n                    self.write_error_message(\n                        f"Latest message (role: {last_message[\'role\']}) was too large and had to be completely truncated to fit context. Original content: \'{original_content[:50]}...\'. This may lead to unexpected behavior."\n                    )\n                else:  # No effective truncation or content was already minimal\n                    self.write_debug_message(\n                        f"Latest message (role: {last_message[\'role\']}) was too large but truncation logic couldn\'t shorten it effectively or content was minimal. Final tokens: {new_tokens_for_last_message}"\n                    )\n        # Phase 3: Role-based culling (original logic, now guarded and uses role for token counting if resizes occur)\n        if history_list:\n            if history_list[0]["role"] == "system":\n                while len(history_list) > 1 and history_list[1]["role"] == "assistant":\n                    popped_msg = history_list.pop(1)\n                    # current_total_tokens -= self.count_tokens(popped_msg["content"], role=popped_msg["role"]) # Optional: keep total tokens accurate\n                    self.write_debug_message(\n                        f"Removed leading assistant message (role: {popped_msg[\'role\']}) after system prompt."\n                    )\n            else:\n                while history_list and history_list[0]["role"] == "assistant":\n                    popped_msg = history_list.pop(0)\n                    # current_total_tokens -= self.count_tokens(popped_msg["content"], role=popped_msg["role"]) # Optional\n                    self.write_debug_message(\n                        f"Removed leading assistant message (role: {popped_msg[\'role\']})."\n                    )\n    def create_empty_history(self, role="user"):\n        return {"role": role, "content": "", "tokens": 0}\n    def create_one_off_prompt_history(self, prompt):\n        return [\n            {\n                "role": "user",\n                "content": prompt,\n                "tokens": self.count_tokens(prompt, role="user"),\n            }\n        ]\n    def create_prompt(self, user_input):\n        return user_input\n    def run_pre_stream_processes(self, user_input):\n        self.write_assistant_thinking_message()\n    def run_stream_processes(self, user_input):\n        # Returns a string of the assistant\'s response\n        prompt = self.create_prompt(user_input)\n        relevant_full_text = self.build_relevant_full_text(prompt)\n        return self.run_basic_chat_stream(prompt, relevant_full_text)\n    def run_post_stream_processes(self, user_input, stream_output):\n        # Returns whether the output should be accepted\n        return True\n    def run_accepted_output_processes(self, user_input, stream_output):\n        # Run processes that should be run if the output is accepted\n        if self.chat_mode and self.verbose:\n            sys.stdout.write(f"Response accepted, continuing...\\n\\n")\n    def run_bad_output_processes(self, user_input, stream_output):\n        # Run processes that should be run if the output is bad\n        if self.chat_mode:\n            sys.stdout.write(f"Response rejected, ignoring...\\n\\n")\n            sys.stdout.flush()\n    def stream_chat(self, user_input):\n        # Main function for streaming assistant chat to the user.\n        retries = 0\n        accepted = False\n        stream_output = ""\n        while retries < self.output_acceptance_retries:\n            self.run_pre_stream_processes(user_input)\n            stream_output = self.run_stream_processes(user_input)\n            accepted = self.run_post_stream_processes(user_input, stream_output)\n            if accepted:\n                break\n            retries += 1\n        if accepted:\n            self.run_accepted_output_processes(user_input, stream_output)\n        else:\n            self.run_bad_output_processes(user_input, stream_output)\n    def run_basic_chat_stream(self, user_input, relevant_full_text):\n        # Add the user input to the chat history\n        user_content = relevant_full_text + user_input\n        self.add_user_history(user_content, user_input)\n        # Remove old messages from the chat history if too large for context\n        self.cull_history()\n        # Get the generator from of the completion\n        completion_generator = self.call_completion(self.chat_history)\n        # Replace the RAG output with the user input. This reduces the size of the history for future prompts.\n        self.chat_history[-1]["content"] = user_input\n        # Display chat history\n        output_history = self.create_empty_history(\n            role="assistant"\n        )  # Set role for assistant\n        if self.chat_mode:\n            if not self.no_color:\n                sys.stdout.write(f"{self.get_color_prefix(Style.BRIGHT, Fore.WHITE)}")\n            sys.stdout.write(f"\\r{\' \' * 36}\\r")\n            if not self.no_color:\n                sys.stdout.write(f"{self.get_color_suffix()}")\n            sys.stdout.flush()\n        output_history = self.run_completion_generator(\n            completion_generator, output_history, True\n        )\n        output_history["content"] = self.remove_thinking_message(\n            output_history["content"]\n        )\n        if self.chat_mode:\n            if not self.no_color:\n                sys.stdout.write(f"{self.get_color_suffix()}")\n            sys.stdout.write("\\n\\n")\n            sys.stdout.flush()\n        # Add the completion to the chat history\n        output_history["tokens"] = self.count_tokens(\n            output_history["content"], role="assistant"\n        )\n        self.chat_history.append(output_history)\n        return output_history["content"]\n    def update_index_and_chunks(self, file_path, new_chunks, new_embeddings):\n        # Remove old chunks and embeddings for this file\n        self.chunks = [chunk for chunk in self.chunks if chunk["filepath"] != file_path]\n        self.chunks.extend(new_chunks)\n        # Find indices of old embeddings\n        old_embedding_indices = [\n            i for i, chunk in enumerate(self.chunks) if chunk["filepath"] == file_path\n        ]\n        if old_embedding_indices:\n            # Convert list to numpy array\n            old_embedding_indices = np.array(old_embedding_indices, dtype=np.int64)\n            # Remove old embeddings from the index\n            self.index.remove_ids(old_embedding_indices)\n        # Add new embeddings to the index\n        if new_embeddings:\n            self.index.add(np.array(new_embeddings))\n    def create_thinking_context(self, write_to_stdout):\n        # Create a context instead of using member variables in case\n        # multiple completions are running in parallel\n        if write_to_stdout and self.hide_thinking and self.chat_mode:\n            if not self.no_color:\n                sys.stdout.write(self.get_color_prefix(Style.BRIGHT, Fore.WHITE))\n            sys.stdout.write("(thinking...)")\n            if not self.no_color:\n                sys.stdout.write(self.get_color_suffix())\n            sys.stdout.flush()\n        return {\n            "thinking_start_finished": not self.hide_thinking,\n            "thinking_end_finished": not self.hide_thinking,\n            "delta_after_thinking_finished": None,\n        }\n    def is_done_thinking(self, context, content):\n        if not context["thinking_start_finished"]:\n            # Before the thinking start pattern is found, do not print output\n            if len(content) > len(self.thinking_start_pattern) + 20:\n                context["thinking_start_finished"] = True\n                if self.thinking_start_pattern not in content:\n                    # If the start pattern is not in the output, it\'s not thinking\n                    context["thinking_end_finished"] = True\n                    context["delta_after_thinking_finished"] = content\n            return False\n        elif not context["thinking_end_finished"]:\n            # While thinking, do not print output\n            if self.thinking_end_pattern in content:\n                context["thinking_end_finished"] = True\n                delta_after_thinking_finished_parts = content.split(\n                    self.thinking_end_pattern\n                )\n                # Get the last part of the string\n                context["delta_after_thinking_finished"] = (\n                    delta_after_thinking_finished_parts[-1]\n                )\n            return False\n        return True\n    def get_extra_delta_after_thinking(self, context, write_to_stdout):\n        # If the thinking is complete, there may be some extra text after the thinking end pattern\n        # This function returns that extra text if it exists.\n        if context["delta_after_thinking_finished"] is not None:\n            output = context["delta_after_thinking_finished"]\n            context["delta_after_thinking_finished"] = None\n            return output\n        return None\n    def run_completion_generator(\n        self, completion_output, output_message, write_to_stdout\n    ):\n        thinking_context = self.create_thinking_context(write_to_stdout)\n        for chunk in completion_output:\n            delta = chunk["choices"][0]["delta"]\n            if "content" in delta and delta["content"] != None:\n                output_message["content"] += delta["content"]\n                if (\n                    self.is_done_thinking(thinking_context, output_message["content"])\n                    and write_to_stdout\n                ):\n                    if not self.no_color and self.chat_mode:\n                        sys.stdout.write(\n                            self.get_color_prefix(Style.BRIGHT, Fore.WHITE)\n                        )\n                    extra_delta_after_thinking = self.get_extra_delta_after_thinking(\n                        thinking_context, write_to_stdout\n                    )\n                    if extra_delta_after_thinking is not None:\n                        # Remove the thinking message from the output now that it\'s complete and\n                        # print the delta after the thinking message\n                        sys.stdout.write(f"\\r{\' \' * 36}\\r")\n                        sys.stdout.write(extra_delta_after_thinking)\n                    sys.stdout.write(delta["content"])\n                    if not self.no_color and self.chat_mode:\n                        sys.stdout.write(self.get_color_suffix())\n                    sys.stdout.flush()\n        return output_message\n    def remove_thinking_message(self, output):\n        # If hide thinking is enabled, remove the thinking message from any string\n        if self.hide_thinking:\n            thinking_start_parts = output.split(self.thinking_start_pattern)\n            if len(thinking_start_parts) > 1:\n                thinking_end_parts = thinking_start_parts[1].split(\n                    self.thinking_end_pattern\n                )\n                if len(thinking_end_parts) > 1:\n                    # This should be the content between <think> and </think>\n                    # The request implies removing the thinking block.\n                    # To remove it and keep text before and after:\n                    # return thinking_start_parts[0] + thinking_end_parts[1]\n                    # The current behavior seems to return the content *inside* the thinking tags.\n                    # Let\'s clarify. The prompt says "remove the thinking message".\n                    # This usually means remove <think>...</think> itself.\n                    # Current code returns thinking_end_parts[0] which is *inside* the tags.\n                    # To remove the block:\n                    return thinking_start_parts[0] + thinking_end_parts[1]\n        # If any condition for hide thinking fails, return the full output\n        return output\n    def run_one_off_completion(self, prompt):\n        one_off_history = self.create_one_off_prompt_history(prompt)\n        completion_generator = self.call_completion(one_off_history)\n        output = self.run_completion_generator(\n            completion_generator,\n            self.create_empty_history(role="assistant"),\n            False,  # Set role for assistant\n        )["content"]\n        return self.remove_thinking_message(output)\nEOF'}}}


--- Result ---
Status: success
Output: {'status': 'success', 'stdout': '', 'stderr': ''}