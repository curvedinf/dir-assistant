# dir-assistant
[![PyPI](https://img.shields.io/pypi/v/dir-assistant)](https://pypi.org/project/dir-assistant/)
[![GitHub license](https://img.shields.io/github/license/curvedinf/dir-assistant)](LICENSE)
[![GitHub last commit](https://img.shields.io/github/last-commit/curvedinf/dir-assistant)](https://github.com/curvedinf/dir-assistant/commits/main)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/dir-assistant)](https://pypi.org/project/dir-assistant/)
[![GitHub stars](https://img.shields.io/github/stars/curvedinf/dir-assistant)](https://github.com/curvedinf/dir-assistant/stargazers)
[![Ko-fi Link](kofi.webp)](https://ko-fi.com/A0A31B6VB6)

Chat with your current directory's files using a local or API LLM.

![(Demo GIF of dir-assistant being run)](demo.gif)
## Features
- **Local & API Model Support:** Works with local GGUF models via `llama-cpp-python` and API-based models (OpenAI, Anthropic, Gemini, etc.) via `LiteLLM`.
- **RAG-Powered Context:** Uses Retrieval-Augmented Generation (RAG) to find and include the most relevant file snippets in the prompt.
- **Context Optimization:** Intelligently reorders and caches file context to reduce API costs and latency.
- **Interactive Chat & Single-Prompt Mode:** Use it for interactive sessions or for one-off questions.
- **File Watching:** Automatically re-indexes files when they change.
- **Git Integration:** Can automatically commit changes to your repository.
## Installation
```shell
pip install dir-assistant
```
To enable all features, including local hardware acceleration:
```shell
pip install dir-assistant[recommended]
dir-assistant platform
```
## Quickstart
### Chat with a Local Model
1.  **Download a model:**
    ```shell
    dir-assistant models download-llm
    ```
2.  **Start the assistant:**
    ```shell
    dir-assistant
    ```
### Chat with an API Model (e.g., OpenAI)
1.  **Get an API key** from your provider (e.g., [OpenAI Platform](https://platform.openai.com/api-keys)).
2.  **Set the API key:**
    ```shell
    dir-assistant setkey OPENAI_API_KEY "your_key_here"
    ```
3.  **Configure the model:**
    ```shell
    dir-assistant config open
    ```
    And set the following:
    ```toml
    [DIR_ASSISTANT]
    ACTIVE_MODEL_IS_LOCAL = false
    [DIR_ASSISTANT.LITELLM_COMPLETION_OPTIONS]
    model = "gpt-4o"
    ```
4.  **Start the assistant:**
    ```shell
    dir-assistant
    ```
## Configuration
You can configure `dir-assistant` by editing its configuration file.
```shell
dir-assistant config open
```
### RAG Caching and Context Optimization
`dir-assistant` includes an advanced system to optimize the context sent to the LLM, aiming to reduce latency and API costs, especially for users who frequently interact with similar sets of files. This system works by caching successful orderings of file "artifacts" (chunks of file content) and reordering them based on your historical usage patterns.
This feature is always on and works in the background. You can fine-tune its behavior using the following settings in your configuration file:
```toml
[DIR_ASSISTANT]
# The percentage of the least relevant files (based on RAG distance) that can
# be replaced by more historically relevant files from the cache.
# A higher value allows more aggressive replacement of less relevant files
# with potentially more useful, historically-used files.
# Range: 0.0 to 1.0. A value of 0.3 means 30% of the initial RAG results
# are considered for replacement.
ARTIFACT_EXCLUDABLE_FACTOR = 0.3
# The time-to-live (in seconds) for a cached context prefix. If a prefix isn't
# used within this time, it's considered expired and won't be used for optimization.
# This prevents outdated context orderings from being used.
# Default is 3600 (1 hour).
API_CONTEXT_CACHE_TTL = 3600
# Weights used by the optimizer to score and reorder file artifacts.
# Adjusting these can change how aggressively the optimizer prioritizes
# different factors when deciding the final order of files in the prompt.
[DIR_ASSISTANT.RAG_OPTIMIZER_WEIGHTS]
# How much to value artifacts that appear frequently in past prompts.
# Higher value prioritizes popular files.
frequency = 1.0
# How much to penalize artifacts for appearing later in past prompts.
# This helps push more consistently important files to the front.
position = 1.0
# How much to value files that have not been modified recently (more stable).
# This assumes that stable, unchanged files are more foundational.
stability = 1.0
# How much to value prefix orderings that have appeared frequently in history.
# This helps reuse successful prompt structures.
historical_hits = 1.0
# How much to value prefix orderings that are currently in the active cache.
# This gives a boost to very recently successful prompt structures.
cache_hits = 1.0
```
### Other Configurations
For details on configuring local models, CGRAG, Git integration, and more, please see the comments in the configuration file generated by `dir-assistant config open`.
## Usage
Start the assistant in your project directory:
```shell
dir-assistant
```
Or for a single question:
```shell
dir-assistant -s "What does this class do?"
```
## Upgrading
If you encounter issues after upgrading, especially with embedding models, clear the cache:
```shell
dir-assistant clear
```
## Acknowledgements
- Local LLMs are run via the fantastic [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) package.
- API LLMS are run using the also fantastic [LiteLLM](https://github.com/BerriAI/litellm) package.
- Special thanks to [Blazed.deals](https://blazed.deals) for sponsoring this project.
